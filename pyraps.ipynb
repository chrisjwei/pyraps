{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# pyraps\n",
    "\n",
    "For our final project, we will be attempting to classify rap styles based on song lyrics. We will be using a subsection of rap music published in the 1990's, where rap style from different geographical regions were distinct, which differs from modern rap music has become more of an almalgamation of the three main rap styles. The two main geographical regions we will be looking at are east-coast (New York City), west-coast (Los Angeles) - we may extend this to further classify more modern movements such as southern (Atlanta) and midwest (Detroit).\n",
    "\n",
    "Our project consists of three parts.\n",
    "\n",
    "1. Data Collection - We will build a database of lyrics from 1990's rap artists and label them based on the rappers style based on geographical location.\n",
    "2. Creating features - We will create features to capture the rhythm and rhyme of a song, as well as the particular lyrical content and vocabulary.\n",
    "3. Training a classifier - Using our features, we will train different types of classifiers and compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Features\n",
    "\n",
    "We will be using two sets of information as features for our machine learning algorithms: lyrical content, in other words the actual words that are being used and the order in which those words occur in, and rhyme patterns along with rhythmic beats, which we will analyze using NLTK.\n",
    "\n",
    "### Lyrical Content\n",
    "\n",
    "The first and easiest way of creating features is by using a TFIDF vectorizer. Given a domain space, our vectorizer can create constant dimension vectors for each document (in this case song). However, since we need to first preprocess our data first, we will move onto the next feature space.\n",
    "\n",
    "### Rhyme Patterns\n",
    "\n",
    "Rhyme patterns are pretty interesting in the way it manifests itself in east-coast versus west-coast rap. East-coast tends to try to create intricate and interlacing rhyme patterns where as west-coast rap focuses more on creating a vibe rather than building intense rhyme structures. We can use this as another feature we can train on to provide better separation\n",
    "\n",
    "Lets take a look at a couple lines from Nas' \"NY State of Mind\", a classic east coast style song\n",
    "\n",
    "1. Rappers I <font color='blue'>monkey</font> <font color='red'>flip em</font> with the <font color='blue'>funky</font> <font color='red'>rhythm</font> I be <font color='red'>kickin'</font>\n",
    "2. <font color='red'>musician</font>, <font color='red'>inflictin</font> <font color='red'>composition</font>\n",
    "3. <font color='green'>of pain</font> I'm like Scarface <font color='red'>sniffin</font> <font color='green'>cocaine</font>\n",
    "4. Holdin a <font color='purple'>M-16</font>, see with the pen <font color='purple'>I'm extreme</font>, now\n",
    "\n",
    "Now lets take a look at a couple lines from 2Pac's California Love, a west coast style song\n",
    "\n",
    "1. Now let me welcome everybody to the wild, wild <font color='red'>west</font>\n",
    "2. A state that's untouchable like Elliot <font color='red'>Ness</font>\n",
    "3. The track hits ya eardrum like a slug to ya <font color='red'>chest</font>\n",
    "4. Pack a <font color='red'>vest</font> for your Jimmy in the city of <font color='red'>sex</font>\n",
    "\n",
    "We can immediately see a difference between the rhyme style between these two styles of rap. East coast tends to have more rhymes in general and focuses a lot more on variety of rhyme patterns interspersed throughout the lines, as opposed to west coast which focuses more on simpler last-word rhymes.\n",
    "\n",
    "How can we do this computationally? We will use CMU's pronuncation dictionary in the NLTK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from nltk.corpus import cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Pronunciation(object):\n",
    "    \n",
    "    CMUDICT = cmudict.dict()\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.word_lower = word.lower()\n",
    "        if self.word_lower in Pronunciation.CMUDICT:\n",
    "            self.pron = Pronunciation.CMUDICT[self.word_lower][0]\n",
    "            self.syllable_loc = [i for i in xrange(len(self.pron)) if self.pron[i][-1].isdigit()]\n",
    "        else:\n",
    "            self.pron = None\n",
    "            self.syllable_loc = None\n",
    "    def __repr__(self):\n",
    "        if self.pron:\n",
    "            pron_repr =  \"/\".join(self.pron)\n",
    "        else:\n",
    "            pron_repr = \"?\"\n",
    "        return \"%s(%s)\" % (self.word,pron_repr)\n",
    "    \n",
    "    def rhyme_group(self):\n",
    "        if self.syllable_loc == []:\n",
    "            return \"/\".join(self.pron)\n",
    "        elif self.pron == None:\n",
    "            return \"UNKNOWN_GROUP\"\n",
    "        else:\n",
    "            return \"/\".join(self.pron[self.syllable_loc[-1]:])\n",
    "    \n",
    "    def __eq__(self,other):\n",
    "        return self.word_lower == other.word_lower\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.word_lower)\n",
    "    \n",
    "        \n",
    "\n",
    "def tokenize(s):\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w[\\w-]*'?[\\w-]*\")\n",
    "    tokenized_lines = [tokenizer.tokenize(line) for line in s.split(\"\\n\") if line]\n",
    "    return tuple([tuple([Pronunciation(token) for token in token_line]) for token_line in tokenized_lines])\n",
    "    \n",
    "\n",
    "nas = '''Rappers I monkey flip em with the funky rhythm I be kicking\\nmusician, inflicting composition\\nof pain '''+\\\n",
    "      '''I'm like Scarface sniffing cocaine\\nHolding a M-16, see with the pen I'm extreme, now\\n\\n'''\n",
    "token_lines = tokenize(nas)\n",
    "for line in token_lines:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define some sort of metric for rhyming words.\n",
    "We know that monkey(M/AH1/NG/K/IY0) rhymes with funky(F/AH1/NG/K/IY0) and is a perfect rhyme. Lets break this down. Monkey has two syllables and thus two stress vowels. These stress vowels mark separations of syllables - monkey can be broken down to (M/AH1/NG) and (K/IY0); funky can be broken down to (F/AH1/NG) and (K/IY0). Immediately, we see that the last two syllables rhyme because they are equal; the NG at the end of 'mon' and 'fun' also add to the rhyme scheme, but the relationship that causes this to be a strong rhyme is equivalence of the last syllable.\n",
    "\n",
    "Lets look at a harder example. flip(F/L/IH1/P), em(EH1/M) as a couple rhymes with rhythm(R/IH1/DH/AH0/M). To simplify things, lets just look at em(EH1/M) and rhythm(R/IH1/DH/AH0/M). This is a weak rhyme because the stress syllables are different but sound the same. This is another complication we need to take into account.\n",
    "\n",
    "Lets implement a quick naive rhyme scheme to see all of our strong rhymes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "def rhyme_groups_naive(tokens):\n",
    "    groups = collections.defaultdict(set)\n",
    "    for line in tokens:\n",
    "        for token in line:\n",
    "            group = token.rhyme_group()\n",
    "            groups[group].add(token)\n",
    "    return dict(groups)\n",
    "\n",
    "strong_rhyme_groups = rhyme_groups_naive(token_lines)\n",
    "\n",
    "for (k,v) in strong_rhyme_groups.iteritems():\n",
    "    if len(v) > 1:\n",
    "        print k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize this to see if it matches our manual rhyme above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import IPython.display, random\n",
    "\n",
    "def random_color():\n",
    "    return \"#%03x\" % random.randint(0, 0xFFF)\n",
    "\n",
    "# get rid of solo groups\n",
    "groups = [[k,v] for (k,v) in strong_rhyme_groups.iteritems() if len(v) > 1 and k != \"UNKNOWN_GROUP\"]\n",
    "print groups\n",
    "# assign colors\n",
    "for group in groups:\n",
    "    group[0] = random_color()\n",
    "# reverse keys and value\n",
    "color_dict = dict(reduce(lambda x,y: x+y,[[(v_i,k) for v_i in v] for [k,v] in groups]))\n",
    "\n",
    "html = \"\"\n",
    "for token_line in token_lines:\n",
    "    for token in token_line:\n",
    "        if token in color_dict:\n",
    "            html += \"<b><font color=%s>%s</font></b> \" % (color_dict[token], token.word)\n",
    "        else:\n",
    "            html += token.word + \" \"\n",
    "    html += \"<br>\"\n",
    "        \n",
    "\n",
    "IPython.display.display_html(html, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Musixmatch API\n",
    "\n",
    "In this section we will now start using the musixmatch api to start scraping some songs and their respective lyrics. We will import the standard python requests library and make calls to the api with our respective apikey that we regestered for. \n",
    "\n",
    "The standard format for the requests will be:\n",
    "\n",
    "\"http://api.musixmatch.com/ws/1.1/method?track_id=?&apikey=?\"\n",
    "\n",
    "where method are the API methods such as \"track.lyrics.get\", \"track.search\", \"chart.atrists.get\", and many others.\n",
    "We need to fill in a track_id for the song and our respective apikey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Function\n",
    "\n",
    "The code below will now scrape the musixmatch database for you. All you need to do is pass in the correct song and title and the function will return the lyrics to you. The musixmatch api has a database full of songs where each song has a corresponding track id. The thing is that if we want the lyrics for a certain song then we need the respective track id. However now we just use the song's respective information to get the track id and then return the lyrics. We first split the artist and title into the correct format for the api call. Then we just use this information for the track id and lyrics following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "class MusixApi:\n",
    "    def __init__(self, apikey):\n",
    "        self.apikey = apikey\n",
    "        self.search_url = \"http://api.musixmatch.com/ws/1.1/track.search\"\n",
    "        self.lyrics_get_url = \"http://api.musixmatch.com/ws/1.1/track.lyrics.get\"\n",
    "        self.artist_search_url = \"http://api.musixmatch.com/ws/1.1/artist.search\"\n",
    "        self.album_get_url = \"http://api.musixmatch.com/ws/1.1/artist.albums.get\"\n",
    "        self.album_tracks_get_url = \"http://api.musixmatch.com/ws/1.1/album.tracks.get\"\n",
    "        self.track_lyrics_get = \"http://api.musixmatch.com/ws/1.1/track.lyrics.get\"\n",
    "        \n",
    "    def search(self, artist, title):\n",
    "        '''\n",
    "        Pass in artist/title and return song lyrics\n",
    "        Basic search capability\n",
    "        '''\n",
    "        \n",
    "        url = self.search_url\n",
    "        params = {\"q_track\": title.lower(),\n",
    "                  \"q_artist\": artist.lower(),\n",
    "                  \"f_has_lyrics\": 1,\n",
    "                  \"apikey\": self.apikey}\n",
    "        song = requests.get(url, params=params).json()\n",
    "        status_code = song[\"message\"][\"header\"][\"status_code\"]\n",
    "        if status_code != 200:\n",
    "            raise Exception(\"Recieved status code %d\" % status_code)\n",
    "        track_id = song['message']['body']['track_list'][0]['track']['track_id']\n",
    "        \n",
    "        url = self.lyrics_get_url\n",
    "        params = {\"track_id\": track_id,\n",
    "                  \"apikey\": self.apikey}\n",
    "        lyrics = requests.get(url, params=params).json()\n",
    "        status_code = lyrics[\"message\"][\"header\"][\"status_code\"]\n",
    "        if status_code != 200:\n",
    "            raise Exception(\"Recieved status code %d\" % status_code)\n",
    "        return lyrics['message']['body']['lyrics']['lyrics_body']\n",
    "    \n",
    "    def artist_id(self, artist):\n",
    "        '''\n",
    "        This function returns the artist ID for an artist\n",
    "        \n",
    "        Input: An album name\n",
    "        Output: A list of all song lyrics for that album\n",
    "        \n",
    "        '''\n",
    "        params = {\"q_artist\": artist.lower(),\n",
    "                  \"page_size\": 5,\n",
    "                  \"apikey\": self.apikey}\n",
    "        url = self.artist_search_url\n",
    "        artist_json = requests.get(url, params=params).json()\n",
    "        status_code = artist_json[\"message\"][\"header\"][\"status_code\"]\n",
    "        if status_code != 200:\n",
    "            raise Exception(\"Recieved status code %d\" % status_code)\n",
    "        artist_list = artist_json['message']['body']['artist_list']\n",
    "        artist_id = artist_list[0]['artist']['artist_id']\n",
    "        return artist_id\n",
    "    \n",
    "    \n",
    "    def all_albums(self, artist_id):\n",
    "        '''\n",
    "        This function returns all the album for a given artist ID\n",
    "        \n",
    "        Input: the ID of an artist\n",
    "        Output: a list of album\n",
    "        '''\n",
    "        \n",
    "        rez = []\n",
    "        url = self.album_get_url\n",
    "        page_num = 1\n",
    "        album_length = 100\n",
    "        while album_length == 100:\n",
    "            params = {\"artist_id\": artist_id,\n",
    "                      \"s_release_date\": \"desc\",\n",
    "                      \"page_size\": 100,\n",
    "                      \"page\": page_num,\n",
    "                      \"g_album_name\": 1,\n",
    "                      \"apikey\": self.apikey}\n",
    "            album_json = requests.get(url, params=params).json()\n",
    "            status_code = album_json[\"message\"][\"header\"][\"status_code\"]\n",
    "            if status_code != 200:\n",
    "                raise Exception(\"Recieved status code %d\" % status_code)\n",
    "            album_list = album_json['message']['body']['album_list']\n",
    "            rez += [album_result[\"album\"] for album_result in album_list]\n",
    "            album_length = len(album_list)\n",
    "            page_num += 1\n",
    "        return rez\n",
    "    \n",
    "    \n",
    "    def all_lyris_in_album(self, album):\n",
    "        '''\n",
    "        Input: An album\n",
    "        Output: All song lyrics for the respective songs in those albums\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        album_id = album[\"album_id\"]\n",
    "        url = self.album_tracks_get_url\n",
    "        song_url = self.track_lyrics_get\n",
    "        params = {\"album_id\": album_id,\n",
    "                  \"page\": 1,\n",
    "                  \"page_size\": 100,\n",
    "                  \"apikey\": self.apikey}\n",
    "        tracks_json = requests.get(url, params=params).json()\n",
    "        status_code = tracks_json[\"message\"][\"header\"][\"status_code\"]\n",
    "        if status_code != 200:\n",
    "            print \"Album track lookup for %d failed with status_code %d\"\\\n",
    "                % (album_id, status_code)\n",
    "            return (None, None, None)\n",
    "        \n",
    "        track_list = tracks_json['message']['body']['track_list']\n",
    "        final_lyrics = []\n",
    "        total = len(track_list)\n",
    "        for track in track_list:\n",
    "            song_id = track['track']['track_id']\n",
    "            song_params = {\"track_id\": song_id,\n",
    "                          \"apikey\": self.apikey}\n",
    "            response = requests.get(song_url, params=song_params).json()\n",
    "            status_code = response['message']['header']['status_code']\n",
    "            if status_code == 200:\n",
    "                final_lyrics.append(response['message']['body']['lyrics']['lyrics_body'])\n",
    "        return (final_lyrics, len(final_lyrics), total)\n",
    "        \n",
    "    def get_all_lyrics_from_artist(self, artist, date_start, date_end):\n",
    "        '''\n",
    "        Input: artist name and the range of album dates we want\n",
    "        Output: List of (album_name, lyrics) from that arist in said date range\n",
    "        '''\n",
    "        def in_date_range(date_string, start, end):\n",
    "            try:\n",
    "                dt = datetime.strptime(date_string, \"%Y-%m-%d\")\n",
    "            except:\n",
    "                try:\n",
    "                    dt = datetime.strptime(date_string, \"%Y-%m\")\n",
    "                except:\n",
    "                    try:\n",
    "                        dt = datetime.strptime(date_string, \"%Y\")\n",
    "                    except:\n",
    "                        return False\n",
    "            return dt <= date_end and dt >= date_start\n",
    "        print \"*******************************************************\"\n",
    "        print artist\n",
    "        print \"*******************************************************\"\n",
    "        artist_id = self.artist_id(artist)\n",
    "        print \" * artist_id: %d\" % artist_id\n",
    "        albums = self.all_albums(artist_id)\n",
    "        print \" * number albums: %d\" % len(albums)\n",
    "        albums_in_range = [album for album in albums if \n",
    "                         in_date_range(album[\"album_release_date\"], date_start, date_end)]\n",
    "        print \" * number albums in date range: %d\" % len(albums_in_range)        \n",
    "        \n",
    "        all_lyrics = []\n",
    "        \n",
    "        for album in albums_in_range:\n",
    "            (lyrics, success, total) = self.all_lyris_in_album(album)\n",
    "            if lyrics == None:\n",
    "                continue\n",
    "            all_lyrics.append((album[\"album_name\"], lyrics))\n",
    "            print \" * found (%d/%d) lyrics in album %s\" % (success, total, album[\"album_name\"])\n",
    "        return all_lyrics\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will get an API key that is stored in a file called 'secrets.json'. For security reasons, it is never a good idea to post any personal keys to the public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"secrets.json\", \"r\") as f:\n",
    "    music_parser = MusixApi(json.load(f)[\"musixApiKey\"])\n",
    "\n",
    "#search(\"Taylor Swift\", \"Back To December\")\n",
    "#print music_parser.search(\"Mobb Deep\", \"Survival of the Fittest\")\n",
    "#ID = music_parser.artist_id(\"Jay-Z\")\n",
    "#albums = music_parser.all_albums(ID)\n",
    "#music_parser.get_all_lyrics_from_artist(\"Coldplay\", datetime(2008,1,1), datetime(2009,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phenomenal! We pretty much have most of the functions we need to start scraping the musixmatch library for all our rap lyrics. We have everything we need. Now we'll just get some real data like a csv file of rapper names! We'll use the rapper names to generate all songs that rapper has created recently. So if we input a csv file of say ['Ice Cube', 'Kanye', ...], then we can return all the rap lyrics for those guys!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a hip-hop lyrics database\n",
    "\n",
    "After meticulous research, we have compiled a list of hip-hop artists from the 90's that are representative of either East-Coast hip-hop or West-Coast hip-hop. In this section, we will scrape the actual data that we will be using for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Lyric(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def _clean(text):\n",
    "        # drop the footer\n",
    "        text = \"\\n\".join(text.split(\"\\n\\n\")[:-1])\n",
    "        return text\n",
    "    \n",
    "    def __init__(self, text, artist, album, label):\n",
    "        self.artist = artist\n",
    "        self.album = album\n",
    "        self.label = label\n",
    "        self.text = Lyric._clean(text)\n",
    "        self.tokens = tokenize(self.text)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"%s/%s: \\\"%s...\\\"\" % (self.artist, self.album, self.text[:10])\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.artist) + hash(self.album) + hash(self.tokens)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "east_coast_rappers = [\"Notorious B.I.G.\", \"Nas\", \"Wu-Tang Clan\", \"Jay-Z\", \"DMX\", \"Rakim\",\n",
    "                      \"Method Man\", \"Busta Rhymes\", \"Run-DMC\", \"Public Enemy\", \"Mobb Deep\",\n",
    "                      \"KRS-One\", \"50 Cent\", \"Big L\", \"LL Cool J\", \"Ghostface Killah\",\n",
    "                      \"Ol' Dirty Bastard\", \"Raekwon\", \"A Tribe Called Quest\",\n",
    "                      \"Big Daddy Kane\",\"Gang Starr\", \"GZA\", \"Redman\", \"Mos Def\", \"Q-Tip\"]\n",
    "west_coast_rappers = [\"2Pac\", \"Ice Cube\", \"Dr. Dre\", \"Snoop Dogg\", \"N.W.A\",\n",
    "                      \"Nate Dogg\", \"Warren G\", \"MC Ren\", \"Eazy-E\", \"Ice-T\", \"Too $hort\", \"Kurupt\",\n",
    "                      \"The Pharcyde\", \"E-40\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "date_start = datetime(1990,1,1)\n",
    "date_end = datetime(1999,12,31)\n",
    "\n",
    "lyrics = []\n",
    "\n",
    "for artist in east_coast_rappers:\n",
    "    for (album_name, album_lyrics) in music_parser.get_all_lyrics_from_artist(artist, date_start, date_end):\n",
    "        for lyric in album_lyrics:\n",
    "            lyrics.append(Lyric(lyric,artist,album_name,\"east\"))\n",
    "for artist in west_coast_rappers:\n",
    "    for (album_name, album_lyrics) in music_parser.get_all_lyrics_from_artist(artist, date_start, date_end):\n",
    "        for lyric in album_lyrics:\n",
    "            lyrics.append(Lyric(lyric,artist,album_name,\"west\"))\n",
    "with open(\"lyrics.pickle\") as f:\n",
    "    pickle.dump(lyrics, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('lyrics.pickle') as f:\n",
    "    all_lyrics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in slang words\n",
    "As you may imagine, rap contains a lot of slang words that do not have an entry in CMUdict. We can fill in the gaps by approximating the pronunciations using [CMU Lextools](http://www.speech.cs.cmu.edu/tools/lextool.html). The following code finds all the unknown words and writes them to a file as input to the lextool. We then need to parse the return dict file from the lextool and refill unknown pronunciations with our approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slang = set()\n",
    "\n",
    "for lyric in all_lyrics:\n",
    "    if len(lyric.tokens) == 0:\n",
    "        continue\n",
    "    for p in reduce(lambda x,y: list(x) + list(y), lyric.tokens):\n",
    "        if p.pron == None:\n",
    "            slang.add(p.word.lower())\n",
    "\n",
    "slang = sorted(list(slang))\n",
    "with open(\"slang.txt\", \"w\") as f:\n",
    "    for word in slang:\n",
    "        f.write(word.encode('utf8') + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Classification\n",
    "\n",
    "An initial approach to classifying rap lyrics by region would simply be to use a tfidf. With the tfidf function that we wrote for homework, let's see if we can come up with anything meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "sw += [\"new\",\"york\",\"california\",\"cali\",\"la\",\"nyc\",\"ny\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide some training data for classification, we must write a function that outputs the region number that the lyrics fall under. I.e, a '1' represents west coast rap lyrics, '2' represents east coast, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_labels(lyrics,**regions):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        List(Lyric) : a list of lyric objects\n",
    "        regions (Lists of artists in particular regions)\n",
    "    Output:\n",
    "        dense int array representing the labels for each example lyric\n",
    "    \"\"\"\n",
    "    region_labels = {\n",
    "        'no region'  : -1,\n",
    "        'west_coast' : 0,\n",
    "        'east_coast' : 1,\n",
    "        'south'      : 2,\n",
    "        'midwest'    : 3\n",
    "    }\n",
    "    result = []\n",
    "    for lyric in lyrics:\n",
    "        if len(lyric.tokens) == 0:\n",
    "            continue\n",
    "        region_name = 'no region'\n",
    "        for region in regions:\n",
    "            if lyric.artist in regions[region]:\n",
    "                region_name = region\n",
    "                break\n",
    "        result.append(region_labels[region_name])\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# split data into sets by artists\n",
    "\n",
    "# split the data based on artists\n",
    "artists_lyrics = {}\n",
    "all_lyrics = filter(lambda lyric: len(lyric.tokens) > 0, all_lyrics)\n",
    "for lyric in all_lyrics:\n",
    "    artists_lyrics[lyric.artist] = artists_lyrics.get(lyric.artist, [])\n",
    "    artists_lyrics[lyric.artist].append(lyric)\n",
    "\n",
    "# split artists\n",
    "num_artists = len(artists_lyrics)\n",
    "train = int(round(0.75*num_artists))\n",
    "artists = artists_lyrics.keys()\n",
    "random.shuffle(artists)\n",
    "artists_tr = artists[:train]\n",
    "artists_val = artists[train:]\n",
    "print \"Found %d artists, taking %d for training, %d for val\" % (len(artists), len(artists_tr), len(artists_val))\n",
    "lyrics_tr = reduce(lambda x,y: x+y, [artists_lyrics[artist] for artist in artists_tr])\n",
    "lyrics_val = reduce(lambda x,y: x+y, [artists_lyrics[artist] for artist in artists_val])\n",
    "print \"Training set: %d, Validation set: %d\" % (len(lyrics_tr), len(lyrics_val))\n",
    "\n",
    "# create features and labels for training and validation set\n",
    "labels_tr = create_labels(lyrics_tr, east_coast=east_coast_rappers, west_coast=west_coast_rappers)\n",
    "docs_tr = [\" \".join(reduce(lambda x,y: x+y, \n",
    "           [[token.word_lower for token in token_line] for token_line in lyric.tokens],\n",
    "           [])) for lyric in lyrics_tr]\n",
    "\n",
    "labels_val = create_labels(lyrics_val, east_coast=east_coast_rappers, west_coast=west_coast_rappers)\n",
    "docs_val = [\" \".join(reduce(lambda x,y: x+y, \n",
    "           [[token.word_lower for token in token_line] for token_line in lyric.tokens],\n",
    "           [])) for lyric in lyrics_val]\n",
    "docs = docs_tr + docs_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pulled from datascience homework\n",
    "\n",
    "# create a tfidf vectorizer with all docs\n",
    "tfidf = TfidfVectorizer(analyzer='word', stop_words=sw).fit(docs)\n",
    "\n",
    "# predict with an svm\n",
    "kernel = 'linear'\n",
    "classifier = sklearn.svm.SVC(kernel=kernel).fit(tfidf.transform(docs_tr),labels_tr)\n",
    "\n",
    "# transform our validation documents and classify\n",
    "X_val = tfidf.transform(docs_val)\n",
    "classifier_score = classifier.score(X_val, labels_val)\n",
    "labels_pred = classifier.predict(X_val)\n",
    "classifier_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Based on Rhyme\n",
    "Something interesting that we can find out is if there is an underlying structural difference between the two styles of rap. Abandoning the goal of trying to build the best classifier, lets see if we can distinguish between the two styles based soley on rhyme. To do this lets recall the previous section on rhyming. We will extract all rhyming words and create a vector representing said rhyme scheme. Each element in the vector will correspond to the order in which the rhyming words occur.\n",
    "\n",
    "For example, if we take \"Yet I'm the mild, money-gettin' style, rollin' foul The versatile, honey-sticking, wild golden child\", we can represent that as the array `[0,1,0,1,0,0,1,0,1,0]` where 0 represents the \"mild\" sounding words and 1 represents the \"money-gettin'\" sounding words. After extracting these features, we can pad them and concatenate to form our features matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lyric_to_rhyme(lyric):\n",
    "    groups = []\n",
    "    token_lines = lyric.tokens\n",
    "    # get all rhyme groups with more than one rhyming word\n",
    "    for k,v in rhyme_groups_naive(token_lines).iteritems():\n",
    "        if k == \"UNKNOWN_GROUP\" or len(v) <= 1:\n",
    "            continue\n",
    "        groups.append(v)\n",
    "    # give each rhyme group a number and flip the dictionary\n",
    "    lookup = {}\n",
    "    for i,words in enumerate(groups):\n",
    "        for word in list(words):\n",
    "            lookup[word] = i\n",
    "    #TODO: implement new lines\n",
    "    return np.array([lookup[token] for token in reduce(lambda x,y: x+y, token_lines) if token in lookup])\n",
    "    \n",
    "def pad_vector(vector, pad, n):\n",
    "    num_padding = n-len(vector)\n",
    "    if num_padding < 0:\n",
    "        raise Exception(\"Size of vector (%d) greater than pad size (%d)\" % (len(vector), n))\n",
    "    return np.pad(vector, [0,num_padding], \"constant\", constant_values=pad)\n",
    "\n",
    "features = [lyric_to_rhyme(lyric) for lyric in all_lyrics]\n",
    "dim = max([len(feature) for feature in features])\n",
    "\n",
    "X_tr = np.array([pad_vector(lyric_to_rhyme(lyric), -1, dim) for lyric in lyrics_tr])\n",
    "X_val = np.array([pad_vector(lyric_to_rhyme(lyric), -1, dim) for lyric in lyrics_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Feature Extraction & Classification\n",
    "The input can be considered a 1D image of some sorts, in that the cells next to eachother are more related to some underlying structure than cells located further away. We can use convolution and pool layers to extract deeper underlying features then use a fully linked layer to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution1D, MaxPooling1D, Flatten, Dropout\n",
    "\n",
    "# based loosely off of https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(32, 32, border_mode='same', input_shape=(dim, 1)))\n",
    "cnn.add(MaxPooling1D(pool_length=2, stride=None, border_mode='valid'))\n",
    "cnn.add(Convolution1D(32, 5, border_mode='same'))\n",
    "cnn.add(MaxPooling1D(pool_length=2, stride=None, border_mode='valid'))\n",
    "cnn.add(Convolution1D(32, 3, border_mode='same'))\n",
    "cnn.add(MaxPooling1D(pool_length=2, stride=None, border_mode='valid'))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(1024, activation=\"relu\"))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(1024, activation=\"relu\"))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "cnn.summary()\n",
    "\n",
    "# https://keras.io/getting-started/sequential-model-guide/\n",
    "cnn.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "cnn.fit(X_tr[:,:,None], labels_tr, nb_epoch=10, batch_size=32, validation_data=(X_val[:,:,None], labels_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Results\n",
    "Here is one of many results we got from running the above code\n",
    "```____________________________________________________________________________________________________\n",
    "Layer (type)                     Output Shape          Param #     Connected to                     \n",
    "====================================================================================================\n",
    "convolution1d_22 (Convolution1D) (None, 483, 32)       1056        convolution1d_input_6[0][0]      \n",
    "____________________________________________________________________________________________________\n",
    "maxpooling1d_22 (MaxPooling1D)   (None, 241, 32)       0           convolution1d_22[0][0]           \n",
    "____________________________________________________________________________________________________\n",
    "convolution1d_23 (Convolution1D) (None, 241, 32)       5152        maxpooling1d_22[0][0]            \n",
    "____________________________________________________________________________________________________\n",
    "maxpooling1d_23 (MaxPooling1D)   (None, 120, 32)       0           convolution1d_23[0][0]           \n",
    "____________________________________________________________________________________________________\n",
    "convolution1d_24 (Convolution1D) (None, 120, 32)       3104        maxpooling1d_23[0][0]            \n",
    "____________________________________________________________________________________________________\n",
    "maxpooling1d_24 (MaxPooling1D)   (None, 60, 32)        0           convolution1d_24[0][0]           \n",
    "____________________________________________________________________________________________________\n",
    "flatten_6 (Flatten)              (None, 1920)          0           maxpooling1d_24[0][0]            \n",
    "____________________________________________________________________________________________________\n",
    "dense_16 (Dense)                 (None, 1024)          1967104     flatten_6[0][0]                  \n",
    "____________________________________________________________________________________________________\n",
    "dropout_11 (Dropout)             (None, 1024)          0           dense_16[0][0]                   \n",
    "____________________________________________________________________________________________________\n",
    "dense_17 (Dense)                 (None, 1024)          1049600     dropout_11[0][0]                 \n",
    "____________________________________________________________________________________________________\n",
    "dropout_12 (Dropout)             (None, 1024)          0           dense_17[0][0]                   \n",
    "____________________________________________________________________________________________________\n",
    "dense_18 (Dense)                 (None, 1)             1025        dropout_12[0][0]                 \n",
    "====================================================================================================\n",
    "Total params: 3027041\n",
    "____________________________________________________________________________________________________\n",
    "Train on 2310 samples, validate on 1031 samples\n",
    "Epoch 1/10\n",
    "2310/2310 [==============================] - 9s - loss: 0.7002 - acc: 0.6506 - val_loss: 0.6520 - val_acc: 0.6566\n",
    "Epoch 2/10\n",
    "2310/2310 [==============================] - 9s - loss: 0.6265 - acc: 0.6701 - val_loss: 0.6904 - val_acc: 0.6256\n",
    "Epoch 3/10\n",
    "2310/2310 [==============================] - 9s - loss: 0.5202 - acc: 0.7420 - val_loss: 0.8487 - val_acc: 0.6576\n",
    "Epoch 4/10\n",
    "2310/2310 [==============================] - 9s - loss: 0.3756 - acc: 0.8247 - val_loss: 1.3008 - val_acc: 0.6547\n",
    "Epoch 5/10\n",
    "2310/2310 [==============================] - 9s - loss: 0.2713 - acc: 0.8857 - val_loss: 1.4730 - val_acc: 0.5606\n",
    "Epoch 6/10\n",
    "2310/2310 [==============================] - 9s - loss: 0.2390 - acc: 0.8996 - val_loss: 1.5849 - val_acc: 0.6285\n",
    "Epoch 7/10\n",
    "2310/2310 [==============================] - 9s - loss: 0.1804 - acc: 0.9195 - val_loss: 1.7476 - val_acc: 0.6178\n",
    "Epoch 8/10\n",
    "2310/2310 [==============================] - 9s - loss: 0.1548 - acc: 0.9338 - val_loss: 2.1233 - val_acc: 0.6227\n",
    "Epoch 9/10\n",
    "2310/2310 [==============================] - 9s - loss: 0.1501 - acc: 0.9381 - val_loss: 2.3608 - val_acc: 0.5849\n",
    "Epoch 10/10\n",
    "2310/2310 [==============================] - 9s - loss: 0.1414 - acc: 0.9411 - val_loss: 2.7952 - val_acc: 0.5810```\n",
    "\n",
    "### Conclusion\n",
    "Looks like classifying raps based on rhyme scheme is harder than expected! We are only able to achieve around 60% classification accuracy with a fairly shallow neural network. Our intuition is that we need a much more complicated neural network to capture the nuances of rhyme schemes. We also need a much larger database of songs to work off of -- this is rather hard to achieve because \"east coast\" vs \"west coast\" rap is highly subjective and a lot of newer artists embody multiple regions of styles and cannot be used in a classification such as this.\n",
    "\n",
    "Additionally, the rhyming function is fairly juvenile and needs much more work. Right now it only pairs strongly rhyming words together which doesn't work very well in this scenario because rappers will often use pseudo-rhymes and often words are pronounced differently to force a rhyme. We would need a much more in depth rhyming algorithm to extract the real structure, which would require a much more powerful processor."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
